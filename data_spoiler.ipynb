{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to spoil a good sentence?\n",
    "1. Locate the sentence with Genitive (or even posessive)\n",
    "2. Change Genitive to a random case\n",
    "\n",
    "БУДЕМ ЛОМАТЬ ПАДЕЖ ПРЯМОГО ОБЪЕКТА"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "from razdel import tokenize\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "from tqdm import tqdm\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from conllu import parse\n",
    "from typing import List, Tuple, Union\n",
    "import glob\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextIlliteracy:\n",
    "    \"\"\"Class for tokenizing text and changing variant of grammatical category\n",
    "       in words of chosen parts of speech\"\"\"\n",
    "    _tokens: List[str]\n",
    "\n",
    "    def __init__(self, text: str) -> None:\n",
    "        \"\"\"Initializes an object\"\"\"\n",
    "        self._text = text\n",
    "        self._tokens = []\n",
    "    \n",
    "    def get_original_text(self) -> str:\n",
    "        return self._text\n",
    "\n",
    "    def tokenize_text(self) -> List[str]:\n",
    "        \"\"\"Tokenizes text\"\"\"\n",
    "        pass\n",
    "\n",
    "    def spoil_text(self, gram: str, postag_list: List[str]) -> str:\n",
    "        \"\"\"Changes grammatical markers to random inside the choosen category\n",
    "           in all words of choosen parts of speech,\n",
    "           if this grammatical category is relevant for such POS\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextIlliteracyRus(TextIlliteracy):\n",
    "    \"\"\"Class tokenize russian text and changes variant of grammatical category\n",
    "       in words of chosen parts of speech.\"\"\"\n",
    "    \n",
    "    _tokens: List[str]\n",
    "    # https://pymorphy2.readthedocs.io/en/stable/user/grammemes.html#grammeme-docs\n",
    "    \n",
    "    __postags = [\"NOUN\", \"ADJF\", \"ADJS\", \"COMP\", \"VERB\", \"INFN\", \"PRTF\", \"PRTS\",\n",
    "                 \"GRND\", \"NUMR\", \"ADVB\", \"NPRO\", \"PRED\", \"PREP\", \"CONJ\", \"PRCL\", \"INTJ\"]\n",
    "    \n",
    "    __grams = {\"nmbr\": [\"sing\", \"plur\"],\n",
    "               \"case\": [\"nomn\", \"gent\", \"datv\", \"accs\", \"ablt\",\n",
    "                        \"loct\", \"voct\", \"gen2\", \"acc2\", \"loc2\"],\n",
    "               \"anim\": [\"anim\", \"inan\"],\n",
    "               \"gndr\": [\"masc\", \"femn\", \"neut\", \"ms-f\"],\n",
    "               \"aspc\": [\"perf\", \"impf\"],\n",
    "               \"trns\": [\"tran\", \"intr\"],\n",
    "               \"pers\": [\"1per\", \"2per\", \"3per\"],\n",
    "               \"tens\": [\"pres\", \"past\", \"futr\"],\n",
    "               \"mood\": [\"indc\", \"impr\"],\n",
    "               \"invl\": [\"incl\", \"excl\"],\n",
    "               \"voic\": [\"actv\", \"pssv\"]}\n",
    "\n",
    "    def tokenize_text(self) -> List[str]:\n",
    "        \"\"\"For russian:\n",
    "           tokenizes text\"\"\"\n",
    "        if self._tokens != []:\n",
    "            tokens = self._tokens\n",
    "        else:\n",
    "            tokens_with_boundaries = list(tokenize(self._text))\n",
    "            # получили список токенов с границами\n",
    "            tokens = [] # список токенов с пробелами в нужных местах\n",
    "            prev_tok_end = 0\n",
    "            for substring in tokens_with_boundaries:\n",
    "                if substring.start != prev_tok_end:\n",
    "                    tokens.append(\" \")\n",
    "                tokens.append(substring.text)\n",
    "                prev_tok_end = substring.stop\n",
    "            self._tokens = tokens\n",
    "        return tokens\n",
    "        \n",
    "    def spoil_text(self, gram: str=\"nmbr\", postag_list: List[str]=__postags, case: str=\"nomn\") -> str:\n",
    "        \"\"\"For russian:\n",
    "           changes grammatical markers to random inside the choosen category\n",
    "           in all words of choosen parts of speech,\n",
    "           if this grammatical category is relevant for such POS\"\"\"\n",
    "        # берёт список частей речи и категорию,\n",
    "        # которую у этих частей речи надо портить рандомными вариантами\n",
    "        \n",
    "        if self._tokens == []:\n",
    "            self._tokens = self.tokenize_text()\n",
    "\n",
    "        tokens = self._tokens\n",
    "\n",
    "        changed_list = []\n",
    "        for tok in tokens:\n",
    "            tok_analysed = morph.parse(tok)[0]\n",
    "            if 'NOUN' == tok_analysed.tag.POS and ('accs' in tok_analysed.tag or 'gent' in tok_analysed.tag):\n",
    "                new_gram_val = random.choice(TextIlliteracyRus.__grams[gram])\n",
    "                \n",
    "                if new_gram_val in tok_analysed.tag:  # делает вероятнось повторения формы чуть меньше\n",
    "                    new_gram_val = random.choice(TextIlliteracyRus.__grams[gram])\n",
    "\n",
    "                if tok_analysed.inflect({new_gram_val}) is not None:\n",
    "                    changed_tok = tok_analysed.inflect({new_gram_val}).word\n",
    "                    if tok[0].isupper():\n",
    "                        # чтобы при изменении буква оставалась заглавной в т.ч. для слова \"Ма́трица\"\n",
    "                        changed_tok = changed_tok[0].upper()+changed_tok[1:]\n",
    "                    changed_list.append(changed_tok)\n",
    "                else:\n",
    "                    changed_list.append(tok)\n",
    "            else:\n",
    "                changed_list.append(tok)\n",
    "        changed_text = \"\".join(changed_list)\n",
    "        return changed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'А я такое видел в репортаже с чемпионатом по питью пиве на скорость.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = TextIlliteracyRus(\"А я такое видел в репортаже с чемпионата по питью пива на скорость.\")\n",
    "t.spoil_text(\"case\", \"accs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIlliteracy(TextIlliteracy):\n",
    "    \"\"\"Class tokenize russian text and changes variant of grammatical category\n",
    "       in words of chosen parts of speech.\"\"\"\n",
    "    \n",
    "    _tokens: List[str]\n",
    "    # https://pymorphy2.readthedocs.io/en/stable/user/grammemes.html#grammeme-docs\n",
    "    \n",
    "    spoiled_words_counter = 0\n",
    "    __postags = [\"NOUN\", \"ADJF\", \"ADJS\", \"COMP\", \"VERB\", \"INFN\", \"PRTF\", \"PRTS\",\n",
    "                 \"GRND\", \"NUMR\", \"ADVB\", \"NPRO\", \"PRED\", \"PREP\", \"CONJ\", \"PRCL\", \"INTJ\"]\n",
    "    \n",
    "    __grams = {\"nmbr\": [\"sing\", \"plur\"],\n",
    "               \"case\": [\"nomn\", \"gent\", \"datv\", \"accs\", \"ablt\",\n",
    "                        \"loct\", \"voct\", \"gen2\", \"acc2\", \"loc2\"],\n",
    "               \"anim\": [\"anim\", \"inan\"],\n",
    "               \"gndr\": [\"masc\", \"femn\", \"neut\", \"ms-f\"],\n",
    "               \"aspc\": [\"perf\", \"impf\"],\n",
    "               \"trns\": [\"tran\", \"intr\"],\n",
    "               \"pers\": [\"1per\", \"2per\", \"3per\"],\n",
    "               \"tens\": [\"pres\", \"past\", \"futr\"],\n",
    "               \"mood\": [\"indc\", \"impr\"],\n",
    "               \"invl\": [\"incl\", \"excl\"],\n",
    "               \"voic\": [\"actv\", \"pssv\"]}\n",
    "\n",
    "    def tokenize_text(self) -> List[str]:\n",
    "        \"\"\"For russian:\n",
    "           tokenizes text\"\"\"\n",
    "        if self._tokens != []:\n",
    "            tokens = self._tokens\n",
    "        else:\n",
    "            tokens_with_boundaries = list(tokenize(self._text))\n",
    "            # получили список токенов с границами\n",
    "            tokens = [] # список токенов с пробелами в нужных местах\n",
    "            prev_tok_end = 0\n",
    "            for substring in tokens_with_boundaries:\n",
    "                if substring.start != prev_tok_end:\n",
    "                    tokens.append(\" \")\n",
    "                tokens.append(substring.text)\n",
    "                prev_tok_end = substring.stop\n",
    "            self._tokens = tokens\n",
    "        tokens = [t for t in tokens if t!= \" \"]\n",
    "        return tokens\n",
    "        \n",
    "    def spoil_text(self, tokens, places, gram: str=\"nmbr\", postag_list: List[str]=__postags) -> str:\n",
    "        \"\"\"For russian:\n",
    "           changes grammatical markers to random inside the choosen category\n",
    "           in all words of choosen parts of speech,\n",
    "           if this grammatical category is relevant for such POS\"\"\"\n",
    "        # берёт список частей речи и категорию,\n",
    "        # которую у этих частей речи надо портить рандомными вариантами\n",
    "        \n",
    "        changed_list = []\n",
    "        for i in range(len(tokens)):\n",
    "            tok = tokens[i]\n",
    "            if i in places:\n",
    "                oring_tok = tok\n",
    "                tok_analysed = morph.parse(tok)[0]\n",
    "                if tok_analysed.tag.POS in postag_list:\n",
    "                    new_gram_val = random.choice(MyIlliteracy.__grams[gram])\n",
    "                    \n",
    "                    if new_gram_val in tok_analysed.tag:  # делает вероятнось повторения формы чуть меньше\n",
    "                        new_gram_val = random.choice(MyIlliteracy.__grams[gram])\n",
    "\n",
    "                    if tok_analysed.inflect({new_gram_val}) is not None:\n",
    "                        changed_tok = tok_analysed.inflect({new_gram_val}).word\n",
    "                        if tok[0].isupper():\n",
    "                            # чтобы при изменении буква оставалась заглавной в т.ч. для слова \"Ма́трица\"\n",
    "                            changed_tok = changed_tok[0].upper()+changed_tok[1:]\n",
    "                        changed_list.append(changed_tok)\n",
    "                        if oring_tok!= changed_tok:\n",
    "                            self.spoiled_words_counter += 1\n",
    "                    else:\n",
    "                        changed_list.append(tok)\n",
    "                    \n",
    "            else:\n",
    "                changed_list.append(tok)\n",
    "        changed_text = \" \".join(changed_list)\n",
    "        return changed_text, self.spoiled_words_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_and_obj_ids_with_conllu(file_path: str) -> List[Tuple[str, List[Union[int, float]]]]:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "\n",
    "    # Parse the CoNLL-U file content\n",
    "    sentences = parse(data)\n",
    "\n",
    "    sentences_and_objs = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = [token['form'] for token in sentence]\n",
    "        sentence_text = words\n",
    "\n",
    "        obj_ids = [token['id']-1 for token in sentence if 'obj' in token['deprel']]\n",
    "        sentences_and_objs.append((sentence_text, obj_ids))\n",
    "\n",
    "    return sentences_and_objs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spoil_data(sentence: tuple) -> str:\n",
    "    text = sentence[0]\n",
    "    obj_ids = sentence[1]\n",
    "    TextIlliteracyRus(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "files = glob.glob('Subtitles/tagged_texts/*/*.txt', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7899"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "social/tagged_texts/vktexts.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:05<00:16,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "social/tagged_texts/LiveJournalPostsandcommentsGICR.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:19<00:21, 10.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "social/tagged_texts/fbtexts.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:21<00:00,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "social/tagged_texts/twtexts.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for file_path in tqdm(files):\n",
    "    print(file_path)\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = file.read().split('\\n\\n')\n",
    "        chunk_size = len(data)//10\n",
    "        for i in range(10):\n",
    "            chunk = data[i*chunk_size:(i+1)*chunk_size]\n",
    "            with open(f'social/chunked_texts/{file_path.split(\"/\")[-1].strip(\".txt\")}_{i}.txt', 'w', encoding='utf-8') as file:\n",
    "                file.write('\\n\\n'.join(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "files = glob.glob('social/chunked_texts/*.txt', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = \"\"\n",
    "spoiled_data = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7899/7899 [21:28<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "746842\n"
     ]
    }
   ],
   "source": [
    "spoiled_words_counter = 0\n",
    "for file_path in tqdm(files):\n",
    "    # print(file_path)\n",
    "    result = extract_sentences_and_obj_ids_with_conllu(file_path)\n",
    "    for r in result:\n",
    "        m = MyIlliteracy(r[0])\n",
    "        spoiled, count = m.spoil_text(r[0], r[1], gram='case')\n",
    "        spoiled_words_counter += count\n",
    "        normal = \" \".join(r[0])\n",
    "        spoiled_data += f\"\\n{spoiled}\"\n",
    "        normal_data += f\"\\n{normal}\"\n",
    "\n",
    "with open('spoiled_data_subtitles.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(spoiled_data)\n",
    "    spoiled_data = \"\"\n",
    "\n",
    "with open('normal_data_subtitles.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(normal_data)\n",
    "    normal_data = \"\"\n",
    "    \n",
    "print(spoiled_words_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MyIlliteracy('Профильные комитеты Совета Федерации рекомендуют палате одобрить законопроект об изменении границ между Москвой и Московской областью.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = extract_sentences_and_obj_ids_with_conllu(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Профильные комитеты Совета Федерации рекомендуют палате одобрить законопроекте об изменении границ между Москвой и Московской областью .'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.spoil_text(result[0][0], result[0][1], gram='case')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Профильные комитеты Совета Федерации рекомендуют палате одобрить законопроект об изменении границ между Москвой и Московской областью .'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(result[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.tokenize_text().index('привлекательность')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ['Профильные', 'комитеты', 'Совета', 'Федерации', 'рекомендуют', 'палате', 'одобрить', 'законопроект', 'об', 'изменении', 'границ', 'между', 'Москвой', 'и', 'Московской', 'областью', '.']\n",
      "Object IDs: [7]\n",
      "\n",
      "Sentence: ['Как', 'отмечается', 'в', 'отзывах', 'комитетов', 'на', 'данный', 'законопроект', ',', 'изменения', 'границ', 'между', 'Москвой', 'и', 'Московской', 'областью', 'позволят', '\"', 'повысить', 'инвестиционную', 'привлекательность', 'как', 'Москвы', ',', 'так', 'и', 'области', ',', 'что', 'крайне', 'важно', 'для', 'экономического', 'и', 'градостроительного', 'развития', '\"', '.']\n",
      "Object IDs: [20]\n",
      "\n",
      "Sentence: ['Соглашение', 'об', 'изменении', 'границ', 'подписано', 'на', 'днях', 'мэром', 'Москвы', 'Сергеем', 'Собяниным', 'и', 'губернатором', 'Московской', 'области', 'Борисом', 'Громовым', '.']\n",
      "Object IDs: []\n",
      "\n",
      "Sentence: ['\"', 'Изменение', 'границы', 'между', 'Москвой', 'и', 'областью', 'носит', 'характер', 'уточнения', ',', 'цель', 'которого', 'придать', 'юридический', 'статус', 'фактически', 'сложившейся', 'ситуации', 'и', 'границе', 'в', 'целом', '.']\n",
      "Object IDs: [8, 15]\n",
      "\n",
      "Sentence: ['Земельные', 'участки', 'Москве', 'передаются', 'общей', 'площадью', '723,46', 'гектара', ',', 'а', 'в', 'область', 'отойдут', 'земли', 'площадью', '328,45', 'гектара', '\"', ',', '-', 'сказал', '\"', 'Интерфаксу', '\"', 'глава', 'комиссии', 'Совета', 'Федерации', 'по', 'жилищной', 'политике', 'и', 'жилищно-коммунальному', 'хозяйству', 'Валерий', 'Парфенов', '.']\n",
      "Object IDs: []\n",
      "\n",
      "Sentence: []\n",
      "Object IDs: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "file_path = '/home/askatasuna/Documents/Diploma/diploma/data/Interfax/texts_tagged/business199005.txt'  # Replace with your actual file path\n",
    "result = extract_sentences_and_obj_ids_with_conllu(file_path)\n",
    "for sentence, obj_ids in result:\n",
    "    print(f\"Sentence: {sentence}\\nObject IDs: {obj_ids}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data += '\\n\\n'.join([r[0] for r in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/askatasuna/Documents/Diploma/splitted/smaller_file_prefix_az.txt',\n",
       " '/home/askatasuna/Documents/Diploma/splitted/smaller_file_prefix_av.txt',\n",
       " '/home/askatasuna/Documents/Diploma/splitted/smaller_file_prefix_bb.txt',\n",
       " '/home/askatasuna/Documents/Diploma/splitted/smaller_file_prefix_at.txt',\n",
       " '/home/askatasuna/Documents/Diploma/splitted/smaller_file_prefix_ad.txt',\n",
       " '/home/askatasuna/Documents/Diploma/splitted/smaller_file_prefix_bc.txt',\n",
       " '/home/askatasuna/Documents/Diploma/splitted/smaller_file_prefix_ao.txt',\n",
       " '/home/askatasuna/Documents/Diploma/splitted/smaller_file_prefix_ar.txt',\n",
       " '/home/askatasuna/Documents/Diploma/splitted/smaller_file_prefix_aa.txt',\n",
       " '/home/askatasuna/Documents/Diploma/splitted/smaller_file_prefix_ag.txt',\n",
       " '/home/askatasuna/Documents/Diploma/splitted/smaller_file_prefix_ap.txt',\n",
       " '/home/askatasuna/Documents/Diploma/splitted/smaller_file_prefix_ax.txt',\n",
       " '/home/askatasuna/Documents/Diploma/splitted/smaller_file_prefix_ab.txt',\n",
       " '/home/askatasuna/Documents/Diploma/splitted/smaller_file_prefix_ai.txt',\n",
       " '/home/askatasuna/Documents/Diploma/splitted/smaller_file_prefix_aj.txt',\n",
       " '/home/askatasuna/Documents/Diploma/splitted/smaller_file_prefix_as.txt',\n",
       " '/home/askatasuna/Documents/Diploma/splitted/smaller_file_prefix_ak.txt',\n",
       " '/home/askatasuna/Documents/Diploma/splitted/smaller_file_prefix_an.txt',\n",
       " '/home/askatasuna/Documents/Diploma/splitted/smaller_file_prefix_ac.txt',\n",
       " '/home/askatasuna/Documents/Diploma/splitted/smaller_file_prefix_ba.txt',\n",
       " '/home/askatasuna/Documents/Diploma/splitted/smaller_file_prefix_bd.txt',\n",
       " '/home/askatasuna/Documents/Diploma/splitted/smaller_file_prefix_am.txt',\n",
       " '/home/askatasuna/Documents/Diploma/splitted/smaller_file_prefix_ae.txt',\n",
       " '/home/askatasuna/Documents/Diploma/splitted/smaller_file_prefix_ah.txt',\n",
       " '/home/askatasuna/Documents/Diploma/splitted/smaller_file_prefix_ay.txt',\n",
       " '/home/askatasuna/Documents/Diploma/splitted/smaller_file_prefix_aw.txt',\n",
       " '/home/askatasuna/Documents/Diploma/splitted/smaller_file_prefix_be.txt',\n",
       " '/home/askatasuna/Documents/Diploma/splitted/smaller_file_prefix_al.txt',\n",
       " '/home/askatasuna/Documents/Diploma/splitted/smaller_file_prefix_af.txt',\n",
       " '/home/askatasuna/Documents/Diploma/splitted/smaller_file_prefix_au.txt',\n",
       " '/home/askatasuna/Documents/Diploma/splitted/smaller_file_prefix_aq.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_to_spoil = glob.glob('/home/askatasuna/Documents/Diploma/splitted/*.txt')\n",
    "files_to_spoil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1050854/1050854 [29:33<00:00, 592.65it/s] \n",
      "100%|██████████| 723038/723038 [28:10<00:00, 427.69it/s]\n",
      "100%|██████████| 1068723/1068723 [49:02<00:00, 363.19it/s]  \n",
      "100%|██████████| 645610/645610 [26:27<00:00, 406.72it/s]\n",
      "100%|██████████| 890998/890998 [27:22<00:00, 542.52it/s] \n",
      "100%|██████████| 1461608/1461608 [27:25<00:00, 888.47it/s] \n",
      "100%|██████████| 826197/826197 [48:51<00:00, 281.82it/s] \n",
      "100%|██████████| 647709/647709 [26:34<00:00, 406.23it/s]\n",
      "100%|██████████| 864481/864481 [27:38<00:00, 521.36it/s]  \n",
      "100%|██████████| 749935/749935 [26:50<00:00, 465.67it/s]\n",
      "100%|██████████| 743207/743207 [48:10<00:00, 257.14it/s]   \n",
      "100%|██████████| 796800/796800 [27:20<00:00, 485.68it/s] \n",
      "100%|██████████| 785212/785212 [30:30<00:00, 428.99it/s]   \n",
      "100%|██████████| 845126/845126 [29:32<00:00, 476.83it/s] \n",
      "100%|██████████| 916179/916179 [28:03<00:00, 544.35it/s]  \n",
      "100%|██████████| 657519/657519 [26:58<00:00, 406.35it/s]\n",
      "100%|██████████| 763933/763933 [29:32<00:00, 431.05it/s]\n",
      "100%|██████████| 829721/829721 [29:38<00:00, 466.44it/s] \n",
      "100%|██████████| 791270/791270 [29:34<00:00, 445.80it/s] \n",
      "100%|██████████| 1043756/1043756 [29:50<00:00, 583.03it/s] \n",
      "100%|██████████| 1434199/1434199 [29:59<00:00, 796.95it/s] \n",
      "100%|██████████| 873756/873756 [30:07<00:00, 483.38it/s]  \n",
      "100%|██████████| 812084/812084 [29:20<00:00, 461.36it/s]\n",
      "100%|██████████| 832936/832936 [30:13<00:00, 459.40it/s]\n",
      "100%|██████████| 884522/884522 [30:13<00:00, 487.85it/s]  \n",
      "100%|██████████| 896149/896149 [30:12<00:00, 494.54it/s]  \n",
      "100%|██████████| 667551/667551 [13:59<00:00, 794.83it/s] \n",
      "100%|██████████| 779593/779593 [29:57<00:00, 433.71it/s]  \n",
      "100%|██████████| 809520/809520 [30:04<00:00, 448.66it/s] \n",
      "100%|██████████| 708841/708841 [27:21<00:00, 431.85it/s]\n",
      "100%|██████████| 707034/707034 [28:43<00:00, 410.14it/s] \n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for f in files_to_spoil:\n",
    "    with open(f, 'r', encoding='utf-8') as file, open(f\"/home/askatasuna/Documents/Diploma/splitted/{count}.txt\", 'w', encoding='utf-8') as out_file:\n",
    "        for line in tqdm(file.read().split('\\n')):\n",
    "            if line == '':\n",
    "                out_file.write('\\n')\n",
    "            else:\n",
    "                m = TextIlliteracyRus(line)\n",
    "                spoiled = m.spoil_text(\"case\", \"accs\")\n",
    "                out_file.write(spoiled+'\\n')\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for f in files_to_spoil:\n",
    "    with open(f, 'r', encoding='utf-8') as file, open(f\"/home/askatasuna/Documents/Diploma/splitted_normal/{count}.txt\", 'w', encoding='utf-8') as out_file:\n",
    "        out_file.write(file.read())\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x81 in position 1572864000: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_ru_convers.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m----> 2\u001b[0m     data_big \u001b[38;5;241m=\u001b[39m \u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_ru_convers_spoiled.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m tqdm(data_big\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)):\n",
      "File \u001b[0;32m/usr/lib/python3.10/codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m data[consumed:]\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x81 in position 1572864000: invalid start byte"
     ]
    }
   ],
   "source": [
    "with open('preprocessed_ru_convers.txt', 'r', encoding='utf-8') as file:\n",
    "    data_big = file.read()\n",
    "    \n",
    "with open('preprocessed_ru_convers_spoiled.txt', 'w', encoding='utf-8') as file:\n",
    "    for line in tqdm(data_big.split('\\n')):\n",
    "        if line == '':\n",
    "            file.write('\\n')\n",
    "        else:\n",
    "            m = TextIlliteracyRus(line)\n",
    "            m.spoil_text(\"case\", \"accs\")\n",
    "            file.write(spoiled+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
